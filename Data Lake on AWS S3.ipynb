{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 - Data Lake on S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make sure that your AWS credentials are loaded as env vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "\n",
    "#Normally this file should be in ~/.aws/credentials\n",
    "config.read_file(open(\"C:\\\\Users\\\\arunk01\\\\Desktop\\\\Udacity\\\\Datalake\\dwh.cfg\"))\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"]= config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"]= config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "                     .config(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:2.7.0\")\\\n",
    "                     .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"s3a://billingarun/Billing.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      "\n",
      "+---------+--------------+--------------------+--------+------------+------------+------------+-------+\n",
      "|      _c0|           _c1|                 _c2|     _c3|         _c4|         _c5|         _c6|    _c7|\n",
      "+---------+--------------+--------------------+--------+------------+------------+------------+-------+\n",
      "|Zone Name| Main Category|        Sub Category|    Year|        Sale|      Demand|  Collection|EFFI(%)|\n",
      "|PUNE ZONE| HT-INDUSTRIAL|    HT-IND.(EXPRESS)|FY 12-13| 1,253.5944 | 1,020.3831 |   967.3706 |105.48%|\n",
      "|PUNE ZONE|P.D. CONSUMERS|     HT-PD CONSUMERS|FY 12-13|        -   |        -   |     0.0692 |  0.00%|\n",
      "|PUNE ZONE| HT-INDUSTRIAL|HT-IND.(NON-EXPRESS)|FY 12-13| 1,508.0498 | 1,131.4004 | 1,101.6759 |102.70%|\n",
      "|PUNE ZONE|P.D. CONSUMERS|     HT-PD CONSUMERS|FY 12-13|    (0.0000)|    (0.1850)|     0.9225 |-20.06%|\n",
      "+---------+--------------+--------------------+--------+------------+------------+------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"s3a://billingarun/Billing.csv\",sep=\",\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Zone Name: string (nullable = true)\n",
      " |-- Main Category: string (nullable = true)\n",
      " |-- Sub Category: string (nullable = true)\n",
      " |-- Year: string (nullable = true)\n",
      " |-- Sale: string (nullable = true)\n",
      " |-- Demand: string (nullable = true)\n",
      " |-- Collection: string (nullable = true)\n",
      " |-- EFFI(%): string (nullable = true)\n",
      "\n",
      "+---------+--------------+--------------------+--------+------------+------------+------------+-------+\n",
      "|Zone Name| Main Category|        Sub Category|    Year|        Sale|      Demand|  Collection|EFFI(%)|\n",
      "+---------+--------------+--------------------+--------+------------+------------+------------+-------+\n",
      "|PUNE ZONE| HT-INDUSTRIAL|    HT-IND.(EXPRESS)|FY 12-13| 1,253.5944 | 1,020.3831 |   967.3706 |105.48%|\n",
      "|PUNE ZONE|P.D. CONSUMERS|     HT-PD CONSUMERS|FY 12-13|        -   |        -   |     0.0692 |  0.00%|\n",
      "|PUNE ZONE| HT-INDUSTRIAL|HT-IND.(NON-EXPRESS)|FY 12-13| 1,508.0498 | 1,131.4004 | 1,101.6759 |102.70%|\n",
      "|PUNE ZONE|P.D. CONSUMERS|     HT-PD CONSUMERS|FY 12-13|    (0.0000)|    (0.1850)|     0.9225 |-20.06%|\n",
      "|PUNE ZONE|        OTHERS|         HT-SEASONAL|FY 12-13|     0.0080 |     0.0277 |        -   |  0.00%|\n",
      "+---------+--------------+--------------------+--------+------------+------------+------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"s3a://udacity-dend/pagila/payment/payment.csv\",sep=\";\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- payment_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- staff_id: integer (nullable = true)\n",
      " |-- rental_id: integer (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- payment_date: string (nullable = true)\n",
      "\n",
      "+----------+-----------+--------+---------+------+--------------------+\n",
      "|payment_id|customer_id|staff_id|rental_id|amount|        payment_date|\n",
      "+----------+-----------+--------+---------+------+--------------------+\n",
      "|     16050|        269|       2|        7|  1.99|2017-01-24 21:40:...|\n",
      "|     16051|        269|       1|       98|  0.99|2017-01-25 15:16:...|\n",
      "|     16052|        269|       2|      678|  6.99|2017-01-28 21:44:...|\n",
      "|     16053|        269|       2|      703|  0.99|2017-01-29 00:58:...|\n",
      "|     16054|        269|       1|      750|  4.99|2017-01-29 08:10:...|\n",
      "+----------+-----------+--------+---------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- payment_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- staff_id: integer (nullable = true)\n",
      " |-- rental_id: integer (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- payment_date: timestamp (nullable = true)\n",
      "\n",
      "+----------+-----------+--------+---------+------+--------------------+\n",
      "|payment_id|customer_id|staff_id|rental_id|amount|        payment_date|\n",
      "+----------+-----------+--------+---------+------+--------------------+\n",
      "|     16050|        269|       2|        7|  1.99|2017-01-24 15:40:...|\n",
      "|     16051|        269|       1|       98|  0.99|2017-01-25 09:16:...|\n",
      "|     16052|        269|       2|      678|  6.99|2017-01-28 15:44:...|\n",
      "|     16053|        269|       2|      703|  0.99|2017-01-28 18:58:...|\n",
      "|     16054|        269|       1|      750|  4.99|2017-01-29 02:10:...|\n",
      "+----------+-----------+--------+---------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import  pyspark.sql.functions as F\n",
    "dfPayment = df.withColumn(\"payment_date\", F.to_timestamp(\"payment_date\"))\n",
    "dfPayment.printSchema()\n",
    "dfPayment.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"s3a://billingarun/Billing.csv\",sep=\",\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Zone Name: string (nullable = true)\n",
      " |-- Main Category: string (nullable = true)\n",
      " |-- Sub Category: string (nullable = true)\n",
      " |-- Year: string (nullable = true)\n",
      " |-- Sale: string (nullable = true)\n",
      " |-- Demand: string (nullable = true)\n",
      " |-- Collection: string (nullable = true)\n",
      " |-- EFFI(%): string (nullable = true)\n",
      "\n",
      "+---------+--------------+--------------------+--------+------------+------------+------------+-------+\n",
      "|Zone Name| Main Category|        Sub Category|    Year|        Sale|      Demand|  Collection|EFFI(%)|\n",
      "+---------+--------------+--------------------+--------+------------+------------+------------+-------+\n",
      "|PUNE ZONE| HT-INDUSTRIAL|    HT-IND.(EXPRESS)|FY 12-13| 1,253.5944 | 1,020.3831 |   967.3706 |105.48%|\n",
      "|PUNE ZONE|P.D. CONSUMERS|     HT-PD CONSUMERS|FY 12-13|        -   |        -   |     0.0692 |  0.00%|\n",
      "|PUNE ZONE| HT-INDUSTRIAL|HT-IND.(NON-EXPRESS)|FY 12-13| 1,508.0498 | 1,131.4004 | 1,101.6759 |102.70%|\n",
      "|PUNE ZONE|P.D. CONSUMERS|     HT-PD CONSUMERS|FY 12-13|    (0.0000)|    (0.1850)|     0.9225 |-20.06%|\n",
      "|PUNE ZONE|        OTHERS|         HT-SEASONAL|FY 12-13|     0.0080 |     0.0277 |        -   |  0.00%|\n",
      "+---------+--------------+--------------------+--------+------------+------------+------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change String to Float and Integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Zone Name: string (nullable = true)\n",
      " |-- Main Category: string (nullable = true)\n",
      " |-- Sub Category: string (nullable = true)\n",
      " |-- Year: string (nullable = true)\n",
      " |-- Sale: string (nullable = true)\n",
      " |-- Demand: float (nullable = true)\n",
      " |-- Collection: string (nullable = true)\n",
      " |-- EFFI(%): string (nullable = true)\n",
      "\n",
      "+---------+--------------+--------------------+--------+------------+--------+------------+-------+\n",
      "|Zone Name| Main Category|        Sub Category|    Year|        Sale|  Demand|  Collection|EFFI(%)|\n",
      "+---------+--------------+--------------------+--------+------------+--------+------------+-------+\n",
      "|PUNE ZONE| HT-INDUSTRIAL|    HT-IND.(EXPRESS)|FY 12-13| 1,253.5944 |    null|   967.3706 |105.48%|\n",
      "|PUNE ZONE|P.D. CONSUMERS|     HT-PD CONSUMERS|FY 12-13|        -   |    null|     0.0692 |  0.00%|\n",
      "|PUNE ZONE| HT-INDUSTRIAL|HT-IND.(NON-EXPRESS)|FY 12-13| 1,508.0498 |    null| 1,101.6759 |102.70%|\n",
      "|PUNE ZONE|P.D. CONSUMERS|     HT-PD CONSUMERS|FY 12-13|    (0.0000)|    null|     0.9225 |-20.06%|\n",
      "|PUNE ZONE|        OTHERS|         HT-SEASONAL|FY 12-13|     0.0080 |   0.008|        -   |  0.00%|\n",
      "|PUNE ZONE|           PWW|    HT-PWW (EXPRESS)|FY 12-13|   153.4125 |153.4125|    76.8519 |102.73%|\n",
      "|PUNE ZONE|P.D. CONSUMERS|     HT-PD CONSUMERS|FY 12-13|        -   |    null|     0.0365 | -0.95%|\n",
      "|PUNE ZONE|           PWW|HT-PWW (NON-EXPRESS)|FY 12-13|    55.6405 | 55.6405|    28.0493 |103.60%|\n",
      "|PUNE ZONE|P.D. CONSUMERS|     HT-PD CONSUMERS|FY 12-13|        -   |    null|        -   |  0.00%|\n",
      "|PUNE ZONE|    COMMERCIAL|       HT-COMMERCIAL|FY 12-13|   498.6377 |498.6377|   629.2983 |103.34%|\n",
      "+---------+--------------+--------------------+--------+------------+--------+------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import  pyspark.sql.functions as F\n",
    "dfPayment = df.withColumn(\"Demand\", df[\"Sale\"].cast(\"float\"))\n",
    "dfPayment.printSchema()\n",
    "dfPayment.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Zone Name: string (nullable = true)\n",
      " |-- Main Category: string (nullable = true)\n",
      " |-- Sub Category: string (nullable = true)\n",
      " |-- Year: string (nullable = true)\n",
      " |-- Sale: string (nullable = true)\n",
      " |-- Demand: float (nullable = true)\n",
      " |-- Collection: string (nullable = true)\n",
      " |-- EFFI(%): string (nullable = true)\n",
      "\n",
      "+---------+--------------+--------------------+--------+------------+--------+------------+-------+\n",
      "|Zone Name| Main Category|        Sub Category|    Year|        Sale|  Demand|  Collection|EFFI(%)|\n",
      "+---------+--------------+--------------------+--------+------------+--------+------------+-------+\n",
      "|PUNE ZONE| HT-INDUSTRIAL|    HT-IND.(EXPRESS)|FY 12-13| 1,253.5944 |    null|   967.3706 |105.48%|\n",
      "|PUNE ZONE|P.D. CONSUMERS|     HT-PD CONSUMERS|FY 12-13|        -   |    null|     0.0692 |  0.00%|\n",
      "|PUNE ZONE| HT-INDUSTRIAL|HT-IND.(NON-EXPRESS)|FY 12-13| 1,508.0498 |    null| 1,101.6759 |102.70%|\n",
      "|PUNE ZONE|P.D. CONSUMERS|     HT-PD CONSUMERS|FY 12-13|    (0.0000)|    null|     0.9225 |-20.06%|\n",
      "|PUNE ZONE|        OTHERS|         HT-SEASONAL|FY 12-13|     0.0080 |  0.0277|        -   |  0.00%|\n",
      "|PUNE ZONE|           PWW|    HT-PWW (EXPRESS)|FY 12-13|   153.4125 | 78.9497|    76.8519 |102.73%|\n",
      "|PUNE ZONE|P.D. CONSUMERS|     HT-PD CONSUMERS|FY 12-13|        -   |    null|     0.0365 | -0.95%|\n",
      "|PUNE ZONE|           PWW|HT-PWW (NON-EXPRESS)|FY 12-13|    55.6405 | 29.0588|    28.0493 |103.60%|\n",
      "|PUNE ZONE|P.D. CONSUMERS|     HT-PD CONSUMERS|FY 12-13|        -   |    null|        -   |  0.00%|\n",
      "|PUNE ZONE|    COMMERCIAL|       HT-COMMERCIAL|FY 12-13|   498.6377 |650.3106|   629.2983 |103.34%|\n",
      "+---------+--------------+--------------------+--------+------------+--------+------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import  pyspark.sql.functions as F\n",
    "dfPayment = df.withColumn(\"Demand\", df[\"Demand\"].cast(\"float\"))\n",
    "dfPayment.printSchema()\n",
    "dfPayment.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|    Year|            demand|\n",
      "+--------+------------------+\n",
      "|FY 12-13| 2509.779991894029|\n",
      "|FY 13-14|2404.7115864753723|\n",
      "+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfPayment.createOrReplaceTempView(\"Demand\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT Year, sum(Demand) as demand\n",
    "    FROM Demand\n",
    "    GROUP by Year\n",
    "    order by demand desc\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType as R, StructField as Fld, DoubleType as Dbl, StringType as Str, IntegerType as Int, DateType as Date\n",
    "paymentSchema = R([\n",
    "    Fld(\"Zone Name\",Str()),\n",
    "    Fld(\"Main Category\",Str()),\n",
    "    Fld(\"Sub Category\",Str()),\n",
    "    Fld(\"Year\",Str()),\n",
    "    Fld(\"Sale\",Dbl()),\n",
    "    Fld(\"Demand\",Dbl()),\n",
    "    Fld(\"Collection\",Dbl()),\n",
    "    Fld(\"EFFI(%)\",Dbl()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPaymentWithSchema = spark.read.csv(\"s3a://billingarun/Billing.csv\",sep=\",\", schema=paymentSchema, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Zone Name: string (nullable = true)\n",
      " |-- Main Category: string (nullable = true)\n",
      " |-- Sub Category: string (nullable = true)\n",
      " |-- Year: string (nullable = true)\n",
      " |-- Sale: double (nullable = true)\n",
      " |-- Demand: double (nullable = true)\n",
      " |-- Collection: double (nullable = true)\n",
      " |-- EFFI(%): double (nullable = true)\n",
      "\n",
      "+---------+--------------+--------------------+--------+------------+------------+------------+-------+\n",
      "|Zone Name| Main Category|        Sub Category|    Year|        Sale|      Demand|  Collection|EFFI(%)|\n",
      "+---------+--------------+--------------------+--------+------------+------------+------------+-------+\n",
      "|PUNE ZONE| HT-INDUSTRIAL|    HT-IND.(EXPRESS)|FY 12-13| 1,253.5944 | 1,020.3831 |   967.3706 |105.48%|\n",
      "|PUNE ZONE|P.D. CONSUMERS|     HT-PD CONSUMERS|FY 12-13|        -   |        -   |     0.0692 |  0.00%|\n",
      "|PUNE ZONE| HT-INDUSTRIAL|HT-IND.(NON-EXPRESS)|FY 12-13| 1,508.0498 | 1,131.4004 | 1,101.6759 |102.70%|\n",
      "|PUNE ZONE|P.D. CONSUMERS|     HT-PD CONSUMERS|FY 12-13|    (0.0000)|    (0.1850)|     0.9225 |-20.06%|\n",
      "|PUNE ZONE|        OTHERS|         HT-SEASONAL|FY 12-13|     0.0080 |     0.0277 |        -   |  0.00%|\n",
      "+---------+--------------+--------------------+--------+------------+------------+------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfPaymentWithSchema.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|       d|           revenue|\n",
      "+--------+------------------+\n",
      "|FY 12-13|2509.7799999999993|\n",
      "|FY 13-14|2404.7116000000005|\n",
      "|    null|              null|\n",
      "+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfPaymentWithSchema.createOrReplaceTempView(\"billing\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT Year as d, sum(Sale) as revenue\n",
    "    FROM billing\n",
    "    GROUP by d\n",
    "    order by revenue desc\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
